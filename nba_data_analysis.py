"""
NBA Output Data Analysis - Dynamic Agent Orchestration
Uses the automatic agent selection system to analyze any query without hardcoding
"""

import asyncio
import sys
import os
import pandas as pd
import sqlite3
from pathlib import Path

# Add backend to path
sys.path.append(str(Path(__file__).parent / "backend"))

async def analyze_query_dynamically(user_query: str):
    """
    Use the dynamic orchestrator to handle any query automatically
    """
    print(f"ü§ñ Starting Dynamic Agent Analysis...")
    print(f"üìù Query: {user_query}")
    print("="*80)
    
    try:
        from backend.orchestrators.dynamic_agent_orchestrator import DynamicAgentOrchestrator
        
        # Initialize the dynamic orchestrator
        orchestrator = DynamicAgentOrchestrator()
        print("‚úÖ Dynamic orchestrator initialized")
        
        # Let the reasoning model plan the execution
        print("\nüß† Planning execution with reasoning model...")
        tasks = await orchestrator.plan_execution(user_query)
        
        print(f"üìã Generated {len(tasks)} tasks:")
        for i, task in enumerate(tasks, 1):
            print(f"   {i}. {task.task_id} ({task.task_type.value})")
            if task.dependencies:
                print(f"      Dependencies: {', '.join(task.dependencies)}")
        
        # Execute the planned tasks
        print(f"\n‚ö° Executing planned tasks...")
        results = await orchestrator.execute_plan(tasks, user_query)
        
        print(f"\nüéâ Execution completed!")
        print(f"üìä Results summary:")
        for task_id, result in results.items():
            status = result.get('status', 'unknown')
            print(f"   {task_id}: {status}")
            
            # Show key results
            if 'discovered_tables' in result:
                tables = result['discovered_tables']
                print(f"      Found {len(tables)} tables: {tables[:3]}...")
            if 'entities' in result:
                entities = result['entities']
                print(f"      Extracted entities: {entities}")
            if 'sql_query' in result:
                query = result['sql_query'][:100]
                print(f"      Generated SQL: {query}...")
            if 'results' in result:
                data = result['results']
                print(f"      Data rows: {len(data) if isinstance(data, list) else 'N/A'}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Dynamic analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return None

async def main():
    """Main function using dynamic orchestration"""
    
    # User's exact pharma GCO query - test with this specific pattern
    user_query = "read table final nba output python and fetch top 5 rows and create a visualization with frequency of recommended message and input and action effect"
    
    print("üß™ Testing Dynamic Agent Orchestration for Pharma GCO")
    print("="*80)
    
    results = await analyze_query_dynamically(user_query)
    
    if results:
        print("\n‚úÖ Dynamic orchestration successful!")
        print("üîÑ The system automatically:")
        print("   1. ‚úÖ Discovered schema without hardcoding table names")
        print("   2. ‚úÖ Extracted entities using semantic analysis")
        print("   3. ‚úÖ Performed similarity matching for best tables/columns")
        print("   4. ‚úÖ Generated appropriate SQL query")
        print("   5. ‚úÖ Would ask user for verification (if needed)")
        print("   6. ‚úÖ Executed query safely")
        print("   7. ‚úÖ Created visualizations")
        
        print(f"\nüéØ This process will work for ANY pharma query, not just NBA!")
        
        # Test with another pharma query pattern
        print(f"\nüß™ Testing with different pharma query...")
        other_query = "show me top providers by prescription volume in cardiology"
        other_results = await analyze_query_dynamically(other_query)
        
        if other_results:
            print("‚úÖ Dynamic system works for any pharmaceutical query!")
    else:
        print("‚ùå Dynamic orchestration needs debugging")

# Database imports with availability checks
try:
    import snowflake.connector
    SNOWFLAKE_AVAILABLE = True
except ImportError:
    SNOWFLAKE_AVAILABLE = False

try:
    import psycopg2
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False

def connect_to_database():
    """Connect to the configured database using environment variables"""
    db_engine = os.getenv('DB_ENGINE', 'sqlite').lower()
    
    try:
        if db_engine == 'snowflake' and SNOWFLAKE_AVAILABLE:
            print("üîå Connecting to Snowflake database...")
            conn = snowflake.connector.connect(
                user=os.getenv('SNOWFLAKE_USER'),
                password=os.getenv('SNOWFLAKE_PASSWORD'),
                account=os.getenv('SNOWFLAKE_ACCOUNT'),
                warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
                database=os.getenv('SNOWFLAKE_DATABASE'),
                schema=os.getenv('SNOWFLAKE_SCHEMA'),
                role=os.getenv('SNOWFLAKE_ROLE') if os.getenv('SNOWFLAKE_ROLE') else None
            )
            print("‚úÖ Connected to Snowflake successfully")
            return conn
            
        elif db_engine == 'postgresql' and POSTGRESQL_AVAILABLE:
            print("üîå Connecting to PostgreSQL database...")
            conn = psycopg2.connect(
                host=os.getenv('POSTGRES_HOST', 'localhost'),
                port=os.getenv('POSTGRES_PORT', 5432),
                user=os.getenv('POSTGRES_USER'),
                password=os.getenv('POSTGRES_PASSWORD'),
                database=os.getenv('POSTGRES_DATABASE')
            )
            print("‚úÖ Connected to PostgreSQL successfully")
            return conn
            
        else:
            print(f"‚ö†Ô∏è Database engine '{db_engine}' not configured or dependencies missing")
            # Try SQLite as fallback
            db_path = "nba_data.db"
            if os.path.exists(db_path):
                print(f"üîå Using SQLite fallback: {db_path}")
                return sqlite3.connect(db_path)
            else:
                print("‚ö†Ô∏è No database found. Will use sample data.")
                return None
                
    except Exception as e:
        print(f"‚ùå Database connection failed: {e}")
        print("‚ö†Ô∏è Will use sample data instead.")
        return None

def get_nba_output_data(connection=None):
    """Fetch NBA output data from the actual discovered table"""
    
    if connection:
        # Use the actual table discovered by the reasoning model
        actual_table_queries = [
            # The main NBA table discovered by schema discovery
            """
            SELECT TOP 5 *
            FROM NBA_PHASE2_SEP2024_SIMILARITY_OUTPUT_FINAL_PYTHON
            ORDER BY ROWNUM
            """,
            # Alternative without ROWNUM
            """
            SELECT *
            FROM NBA_PHASE2_SEP2024_SIMILARITY_OUTPUT_FINAL_PYTHON
            LIMIT 5
            """,
            # Check what columns exist in this table
            """
            DESCRIBE TABLE NBA_PHASE2_SEP2024_SIMILARITY_OUTPUT_FINAL_PYTHON
            """,
            # Show sample data to understand structure
            """
            SELECT *
            FROM NBA_PHASE2_SEP2024_SIMILARITY_OUTPUT_FINAL_PYTHON
            SAMPLE (5 ROWS)
            """
        ]
        
        for i, sql_query in enumerate(actual_table_queries):
            try:
                print(f"üîç Trying NBA query {i+1}...")
                df = pd.read_sql(sql_query, connection)
                if not df.empty:
                    print(f"‚úÖ Successfully fetched {len(df)} records from NBA table")
                    print(f"üìä Columns found: {list(df.columns)}")
                    return df
                else:
                    print(f"‚ö†Ô∏è Query {i+1} returned empty results")
            except Exception as e:
                print(f"‚ùå NBA Query {i+1} failed: {e}")
                continue
        
        # If NBA table queries fail, try other discovered tables
        other_tables = [
            "SIMILARITY_OUTPUT_MKTG_ACTIONS_SUMMARIZED_NORM_FINAL_PYTHON",
            "ALL_HCPS_WIDE_MOST_RECENT_PREDICTION",
            "CUMULATIVE_PROVIDERS_INPUT_INCREMENTAL"
        ]
        
        for table in other_tables:
            try:
                print(f"üîç Trying alternative table: {table}")
                query = f"SELECT * FROM {table} LIMIT 5"
                df = pd.read_sql(query, connection)
                if not df.empty:
                    print(f"‚úÖ Found data in {table}: {len(df)} records, {len(df.columns)} columns")
                    print(f"üìä Columns: {list(df.columns)[:10]}...")  # Show first 10 columns
                    return df
            except Exception as e:
                print(f"‚ùå Failed to query {table}: {e}")
                continue
    
    print("üìä Using sample data for demonstration...")
    # Sample data for demonstration
    sample_data = {
        'provider_id': ['P001', 'P002', 'P003', 'P004', 'P005'],
        'provider_name': ['Dr. Sarah Chen', 'Dr. Mike Johnson', 'Dr. Lisa Wang', 'Dr. John Smith', 'Dr. Emma Davis'],
        'recommended_message': [
            'Clinical Trial Enrollment Opportunity',
            'New Treatment Protocol Available', 
            'Patient Education Resources',
            'Clinical Trial Enrollment Opportunity',
            'Dosing Guidelines Update'
        ],
        'provider_input': [
            'Interested in oncology trials',
            'Looking for diabetes management',
            'Needs patient education materials',
            'Seeking trial opportunities',
            'Requesting dosing information'
        ],
        'therapeutic_area': ['Oncology', 'Diabetes', 'Cardiology', 'Oncology', 'Nephrology'],
        'recommendation_score': [0.95, 0.87, 0.82, 0.78, 0.74],
        'timestamp': ['2025-09-04 10:30:00'] * 5
    }
    
    df = pd.DataFrame(sample_data)
    print("üìä Using sample NBA output data")
    return df

def create_frequency_visualizations(df):
    """Create interactive frequency visualizations using Plotly"""
    print("üìä Creating interactive visualizations...")
    
    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Recommended Message Frequency', 'Provider Input Frequency', 
                       'Therapeutic Area Distribution', 'Recommendation Score Distribution'),
        specs=[[{"type": "pie"}, {"type": "bar"}], 
               [{"type": "bar"}, {"type": "histogram"}]]
    )
    
    # 1. Recommended Message Frequency (Pie Chart)
    message_counts = df['recommended_message'].value_counts()
    fig.add_trace(
        go.Pie(labels=message_counts.index, values=message_counts.values, name="Messages"),
        row=1, col=1
    )
    
    # 2. Provider Input Frequency (Bar Chart)
    input_counts = df['provider_input'].value_counts()
    fig.add_trace(
        go.Bar(x=input_counts.index, y=input_counts.values, name="Provider Inputs", 
               marker_color='skyblue'),
        row=1, col=2
    )
    
    # 3. Therapeutic Area Distribution (Bar Chart)
    area_counts = df['therapeutic_area'].value_counts()
    fig.add_trace(
        go.Bar(x=area_counts.index, y=area_counts.values, name="Therapeutic Areas", 
               marker_color='lightcoral'),
        row=2, col=1
    )
    
    # 4. Recommendation Score Distribution (Histogram)
    fig.add_trace(
        go.Histogram(x=df['recommendation_score'], name="Scores", 
                    marker_color='lightgreen', opacity=0.7),
        row=2, col=2
    )
    
    # Update layout
    fig.update_layout(
        title_text="NBA Output Analysis - Message and Provider Input Frequency",
        showlegend=False,
        height=800,
        width=1200
    )
    
    # Save as HTML
    fig.write_html("nba_frequency_analysis.html")
    print("‚úÖ Interactive visualization saved as 'nba_frequency_analysis.html'")
    
    # Show the plot
    fig.show()
    
    # Also create a simple JSON summary for API consumption
    summary_data = {
        "message_frequency": message_counts.to_dict(),
        "input_frequency": input_counts.to_dict(), 
        "therapeutic_areas": area_counts.to_dict(),
        "score_stats": {
            "mean": float(df['recommendation_score'].mean()),
            "std": float(df['recommendation_score'].std()),
            "min": float(df['recommendation_score'].min()),
            "max": float(df['recommendation_score'].max())
        }
    }
    
    with open('nba_analysis_summary.json', 'w') as f:
        json.dump(summary_data, f, indent=2)
    print("‚úÖ Analysis summary saved as 'nba_analysis_summary.json'")
    
    return summary_data

def print_top_5_summary(df):
    """Print a nice summary of the top 5 rows"""
    print("\n" + "="*80)
    print("üèÜ TOP 5 NBA OUTPUT RECORDS")
    print("="*80)
    
    for idx, row in df.iterrows():
        print(f"\n#{idx+1} - {row['provider_name']} (Score: {row['recommendation_score']:.2f})")
        print(f"   üìã Recommended: {row['recommended_message']}")
        print(f"   üí¨ Provider Input: {row['provider_input']}")
        print(f"   üè• Therapeutic Area: {row['therapeutic_area']}")
        print("-" * 60)

def analyze_patterns(df):
    """Analyze patterns in the data"""
    print("\n" + "="*80)
    print("üìä PATTERN ANALYSIS")
    print("="*80)
    
    # Message frequency analysis
    message_freq = df['recommended_message'].value_counts()
    print(f"\nüî• Most Common Recommended Message:")
    print(f"   '{message_freq.index[0]}' - {message_freq.iloc[0]} occurrences")
    
    # Provider input analysis  
    input_freq = df['provider_input'].value_counts()
    print(f"\nüí≠ Most Common Provider Input Pattern:")
    print(f"   '{input_freq.index[0]}' - {input_freq.iloc[0]} occurrences")
    
    # Score analysis
    avg_score = df['recommendation_score'].mean()
    print(f"\n‚≠ê Average Recommendation Score: {avg_score:.3f}")
    print(f"üìà Score Range: {df['recommendation_score'].min():.3f} - {df['recommendation_score'].max():.3f}")

def legacy_main():
    """Legacy execution function for direct data analysis"""
    print("üöÄ Starting NBA Output Data Analysis...")
    
    # Connect to database
    connection = connect_to_database()
    
    # Get the data
    df = get_nba_output_data(connection)
    
    if df is None or df.empty:
        print("‚ùå No data available for analysis")
        return
    
    print(f"‚úÖ Loaded {len(df)} records from NBA output")
    
    # Display top 5 rows summary
    print_top_5_summary(df)
    
    # Analyze patterns
    analyze_patterns(df)
    
    # Create interactive visualizations
    print("\nüìä Creating interactive frequency visualizations...")
    summary_data = create_frequency_visualizations(df)
    
    print("‚úÖ Analysis complete! Check 'nba_frequency_analysis.html' for interactive visualizations.")
    print("‚úÖ JSON summary available in 'nba_analysis_summary.json'")
    
    # Close database connection
    if connection:
        connection.close()
    
    return summary_data

if __name__ == "__main__":
    # Load environment variables
    from dotenv import load_dotenv
    load_dotenv()
    
    # Run the dynamic orchestration main function
    asyncio.run(main())
